[
  {
    "name": "CloudCredentialOperatorTargetNamespaceMissing",
    "query": "cco_credentials_requests_conditions{condition=\"MissingTargetNamespace\"} > 0",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "summary": "CredentialsRequest(s) pointing to non-existant namespace"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "CloudCredentialOperatorProvisioningFailed",
    "query": "cco_credentials_requests_conditions{condition=\"CredentialsProvisionFailure\"} > 0",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "summary": "CredentialsRequest(s) unable to be fulfilled"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "CloudCredentialOperatorDeprovisioningFailed",
    "query": "cco_credentials_requests_conditions{condition=\"CredentialsDeprovisionFailure\"} > 0",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "summary": "CredentialsRequest(s) unable to be cleaned up"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "CloudCredentialOperatorInsufficientCloudCreds",
    "query": "cco_credentials_requests_conditions{condition=\"InsufficientCloudCreds\"} > 0",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "summary": "Cluster's cloud credentials insufficient for minting or passthrough"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "CloudCredentialOperatorDown",
    "query": "absent(up{job=\"cco-metrics\"} == 1)",
    "duration": 300,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "summary": "cloud-credential-operator pod not running"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClusterMachineApproverDown",
    "query": "absent(up{job=\"machine-approver\"} == 1)",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "ClusterMachineApprover has disappeared from Prometheus target discovery."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "MachineApproverMaxPendingCSRsReached",
    "query": "mapi_current_pending_csr > mapi_max_pending_csr",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "max pending CSRs threshold reached."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "SamplesDegraded",
    "query": "openshift_samples_degraded_info == 1",
    "duration": 7200,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Samples could not be deployed and are in Degraded status. You can look at the \"openshift-samples\" ClusterOperator object for details. You can also query 'openshift_samples_failed_imagestream_import_info' to see if ImageStreams do not import. The samples operator reports itself Degraded if an ImageStream import continues to fail for 2 hours.\n"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "SamplesInvalidConfig",
    "query": "openshift_samples_invalidconfig_info == 1",
    "duration": 7200,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Samples operator has been given an invalid configuration.\n"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "SamplesMissingSecret",
    "query": "openshift_samples_invalidsecret_info{reason=\"missing_secret\"} == 1",
    "duration": 7200,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Samples operator cannot find the samples pull secret in the openshift namespace.\n"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "SamplesMissingTBRCredential",
    "query": "openshift_samples_invalidsecret_info{reason=\"missing_tbr_credential\"} == 1",
    "duration": 7200,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Samples operator cannot find credentials for 'registry.redhat.io' in the samples pull secret in the openshift namespace.  Unless you have updated the 'samplesRegistry' field in the samples operator config to change which registry the sample ImageStreams are pulled from, you will have failed imports for many of the sample ImageStreams in this case.\n"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClusterOperatorDown",
    "query": "cluster_operator_up{job=\"cluster-version-operator\"} == 0",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Cluster operator {{ $labels.name }} has not been available for 10 mins. Operator may be down or disabled, cluster will not be kept up to date and upgrades will not be possible."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClusterOperatorDegraded",
    "query": "cluster_operator_conditions{condition=\"Degraded\",job=\"cluster-version-operator\"} == 1",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Cluster operator {{ $labels.name }} has been degraded for 10 mins. Operator is degraded because {{ $labels.reason }} and cluster upgrades will be unstable."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClusterOperatorFlapping",
    "query": "changes(cluster_operator_up{job=\"cluster-version-operator\"}[2m]) > 2",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Cluster operator {{ $labels.name }} up status is changing often. This might cause upgrades to be unstable."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClusterVersionOperatorDown",
    "query": "absent(up{job=\"cluster-version-operator\"} == 1)",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Cluster version operator has disappeared from Prometheus target discovery. Operator may be down or disabled, cluster will not be kept up to date and upgrades will not be possible."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ImageRegistryStorageReconfigured",
    "query": "increase(image_registry_operator_storage_reconfigured_total[30m]) > 0",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Image Registry Storage configuration has changed in the last 30\nminutes. This change may have caused data loss.\n"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ImageRegistryRemoved",
    "query": "cluster_operator_conditions{condition=\"Available\",name=\"image-registry\",reason=\"Removed\"} > 0",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Image Registry has been removed. ImageStreamTags, BuildConfigs and\nDeploymentConfigs which reference ImageStreamTags may not work as\nexpected. Please configure storage and update the config to Managed\nstate by editing configs.imageregistry.operator.openshift.io.\n"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "TechPreviewNoUpgrade",
    "query": "cluster_feature_set{name!=\"\",namespace=\"openshift-kube-apiserver-operator\"} == 0",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Cluster has enabled tech preview features that will prevent upgrades."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PodDisruptionBudgetAtLimit",
    "query": "kube_poddisruptionbudget_status_expected_pods == on(namespace, poddisruptionbudget, service) kube_poddisruptionbudget_status_desired_healthy",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "The pod disruption budget is preventing further disruption to pods because it is at the minimum allowed level."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PodDisruptionBudgetLimit",
    "query": "kube_poddisruptionbudget_status_expected_pods < on(namespace, poddisruptionbudget, service) kube_poddisruptionbudget_status_desired_healthy",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "The pod disruption budget is below the minimum number allowed pods."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeSchedulerDown",
    "query": "absent(up{job=\"scheduler\"} == 1)",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "KubeScheduler has disappeared from Prometheus target discovery."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClusterAutoscalerOperatorDown",
    "query": "absent(up{job=\"cluster-autoscaler-operator\"} == 1)",
    "duration": 300,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "cluster-autoscaler-operator has disappeared from Prometheus target discovery."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "MachineAPIOperatorDown",
    "query": "absent(up{job=\"machine-api-operator\"} == 1)",
    "duration": 180,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "machine api operator is down"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "MachineAPIOperatorMetricsCollectionFailing",
    "query": "mapi_mao_collector_up == 0",
    "duration": 180,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "machine api operator metrics collection is failing. For more details:  oc logs <machine-api-operator-pod-name> -n openshift-machine-api"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "MachineWithNoRunningPhase",
    "query": "(mapi_machine_created_timestamp_seconds{phase!=\"Running\"}) > 0",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "machine {{ $labels.name }} is in {{ $labels.phase }} phase"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "MachineWithoutValidNode",
    "query": "(mapi_machine_created_timestamp_seconds unless on(node) kube_node_info) > 0",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "machine {{ $labels.name }} does not have valid node reference"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "MCDDrainError",
    "query": "mcd_drain > 0",
    "duration": 0,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Drain failed on {{ $labels.node }} , updates may be blocked. For more details:  oc logs -f -n openshift-machine-config-operator machine-config-daemon-<hash> -c machine-config-daemon"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeletHealthState",
    "query": "mcd_kubelet_state > 2",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Kubelet health failure threshold reached"
    },
    "alerts": [
      {
        "labels": {
          "alertname": "KubeletHealthState",
          "endpoint": "metrics",
          "err": "kubelet health failure threshold reached",
          "instance": "192.168.81.151:9001",
          "job": "machine-config-daemon",
          "namespace": "openshift-machine-config-operator",
          "pod": "machine-config-daemon-vf6xn",
          "service": "machine-config-daemon",
          "severity": "warning"
        },
        "annotations": {
          "message": "Kubelet health failure threshold reached"
        },
        "state": "firing",
        "activeAt": "2020-04-18T09:48:22.339491658Z",
        "value": "3e+00"
      }
    ],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "MCDPivotError",
    "query": "mcd_pivot_err > 0",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Error detected in pivot logs on {{ $labels.node }} "
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "MCDRebootError",
    "query": "mcd_reboot_err > 0",
    "duration": 0,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Reboot failed on {{ $labels.node }} , update may be blocked"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "CertifiedOperatorConnectionErrors",
    "query": "rate(app_registry_request_total{code!~\"2..\",opsrc=\"certified-operators\"}[5m]) / rate(app_registry_request_total{opsrc=\"certified-operators\"}[5m]) >= 0.5",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Unable to connect to the default OperatorSource certified-operators AppRegistry for {{ $value }}% of requests."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "CommunityOperatorConnectionErrors",
    "query": "rate(app_registry_request_total{code!~\"2..\",opsrc=\"community-operators\"}[5m]) / rate(app_registry_request_total{opsrc=\"community-operators\"}[5m]) >= 0.5",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Unable to connect to the default OperatorSource community-operators AppRegistry for {{ $value }}% of requests."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "RedhatOperatorConnectionErrors",
    "query": "rate(app_registry_request_total{code!~\"2..\",opsrc=\"redhat-operators\"}[5m]) / rate(app_registry_request_total{opsrc=\"redhat-operators\"}[5m]) >= 0.5",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Unable to connect to the default OperatorSource redhat-operators AppRegistry for {{ $value }}% of requests."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "AlertmanagerConfigInconsistent",
    "query": "count_values by(service) (\"config_hash\", alertmanager_config_hash{job=\"alertmanager-main\",namespace=\"openshift-monitoring\"}) / on(service) group_left() label_replace(max by(name, job, namespace, controller) (prometheus_operator_spec_replicas{controller=\"alertmanager\",job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}), \"service\", \"alertmanager-$1\", \"name\", \"(.*)\") != 1",
    "duration": 300,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "The configuration of the instances of the Alertmanager cluster `{{$labels.service}}` are out of sync."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "AlertmanagerFailedReload",
    "query": "alertmanager_config_last_reload_successful{job=\"alertmanager-main\",namespace=\"openshift-monitoring\"} == 0",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "AlertmanagerMembersInconsistent",
    "query": "alertmanager_cluster_members{job=\"alertmanager-main\",namespace=\"openshift-monitoring\"} != on(service) group_left() count by(service) (alertmanager_cluster_members{job=\"alertmanager-main\",namespace=\"openshift-monitoring\"})",
    "duration": 300,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Alertmanager has not found all other members of the cluster."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdMembersDown",
    "query": "max by(job) (sum by(job) (up{job=~\".*etcd.*\"} == bool 0) or count by(job, endpoint) (sum by(job, endpoint, To) (rate(etcd_network_peer_sent_failures_total{job=~\".*etcd.*\"}[3m])) > 0.01)) > 0",
    "duration": 180,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": members are down ({{ $value }})."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdInsufficientMembers",
    "query": "sum by(job) (up{job=~\".*etcd.*\"} == bool 1) < ((count by(job) (up{job=~\".*etcd.*\"}) + 1) / 2)",
    "duration": 180,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": insufficient members ({{ $value }})."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdNoLeader",
    "query": "etcd_server_has_leader{job=~\".*etcd.*\"} == 0",
    "duration": 60,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": member {{ $labels.instance }} has no leader."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdHighNumberOfLeaderChanges",
    "query": "increase((max by(job) (etcd_server_leader_changes_seen_total{job=~\".*etcd.*\"}) or 0 * absent(etcd_server_leader_changes_seen_total{job=~\".*etcd.*\"}))[15m:1m]) >= 3",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": {{ $value }} leader changes within the last 15 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdGRPCRequestsSlow",
    "query": "histogram_quantile(0.99, sum by(job, instance, grpc_service, grpc_method, le) (rate(grpc_server_handling_seconds_bucket{grpc_type=\"unary\",job=~\".*etcd.*\"}[5m]))) > 0.15",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": gRPC requests to {{ $labels.grpc_method }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdMemberCommunicationSlow",
    "query": "histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.15",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdHighNumberOfFailedProposals",
    "query": "rate(etcd_server_proposals_failed_total{job=~\".*etcd.*\"}[15m]) > 5",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": {{ $value }} proposal failures within the last 30 minutes on etcd instance {{ $labels.instance }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdHighFsyncDurations",
    "query": "histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.5",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": 99th percentile fync durations are {{ $value }}s on etcd instance {{ $labels.instance }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdHighCommitDurations",
    "query": "histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.25",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "etcd cluster \"{{ $labels.job }}\": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdHighNumberOfFailedHTTPRequests",
    "query": "sum by(method) (rate(etcd_http_failed_total{code!=\"404\",job=~\".*etcd.*\"}[5m])) / sum by(method) (rate(etcd_http_received_total{job=~\".*etcd.*\"}[5m])) > 0.01",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance {{ $labels.instance }}"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdHighNumberOfFailedHTTPRequests",
    "query": "sum by(method) (rate(etcd_http_failed_total{code!=\"404\",job=~\".*etcd.*\"}[5m])) / sum by(method) (rate(etcd_http_received_total{job=~\".*etcd.*\"}[5m])) > 0.05",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance {{ $labels.instance }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "etcdHTTPRequestsSlow",
    "query": "histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m])) > 0.15",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method }} are slow."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "TargetDown",
    "query": "100 * (count by(job, namespace, service) (up == 0) / count by(job, namespace, service) (up)) > 10",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "{{ printf \"%.4g\" $value }}% of the {{ $labels.job }} targets in {{ $labels.namespace }} namespace are down."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "Watchdog",
    "query": "vector(1)",
    "duration": 0,
    "labels": {
      "severity": "none"
    },
    "annotations": {
      "message": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n"
    },
    "alerts": [
      {
        "labels": {
          "alertname": "Watchdog",
          "severity": "none"
        },
        "annotations": {
          "message": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n"
        },
        "state": "firing",
        "activeAt": "2020-04-18T09:46:00.163677339Z",
        "value": "1e+00"
      }
    ],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubePodCrashLooping",
    "query": "rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}[15m]) * 60 * 5 > 0",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubePodNotReady",
    "query": "sum by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",phase=~\"Pending|Unknown\"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!=\"Job\"})) > 0",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeDeploymentGenerationMismatch",
    "query": "kube_deployment_status_observed_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeDeploymentReplicasMismatch",
    "query": "kube_deployment_spec_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_deployment_status_replicas_available{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeStatefulSetReplicasMismatch",
    "query": "kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeStatefulSetGenerationMismatch",
    "query": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeStatefulSetUpdateNotRolledOut",
    "query": "max without(revision) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) * (kube_statefulset_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"})",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeDaemonSetRolloutStuck",
    "query": "kube_daemonset_status_number_ready{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} / kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} < 1",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Only {{ $value | humanizePercentage }} of the desired Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeContainerWaiting",
    "query": "sum by(namespace, pod, container) (kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) > 0",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container}} has been in waiting state for longer than 1 hour."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeDaemonSetNotScheduled",
    "query": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 0",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeDaemonSetMisScheduled",
    "query": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 0",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeCronJobRunning",
    "query": "time() - kube_cronjob_next_schedule_time{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 3600",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeJobCompletion",
    "query": "kube_job_spec_completions{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} - kube_job_status_succeeded{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 0",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than one hour to complete."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeJobFailed",
    "query": "kube_job_failed{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} > 0",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeHpaReplicasMismatch",
    "query": "(kube_hpa_status_desired_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} != kube_hpa_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) and changes(kube_hpa_status_current_replicas[15m]) == 0",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the desired number of replicas for longer than 15 minutes."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeHpaMaxedOut",
    "query": "kube_hpa_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} == kube_hpa_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at max replicas for longer than 15 minutes."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeCPUOvercommit",
    "query": "sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum) / sum(kube_node_status_allocatable_cpu_cores) > (count(kube_node_status_allocatable_cpu_cores) - 1) / count(kube_node_status_allocatable_cpu_cores)",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeMemOvercommit",
    "query": "sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum) / sum(kube_node_status_allocatable_memory_bytes) > (count(kube_node_status_allocatable_memory_bytes) - 1) / count(kube_node_status_allocatable_memory_bytes)",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeCPUOvercommit",
    "query": "sum(kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",resource=\"cpu\",type=\"hard\"}) / sum(kube_node_status_allocatable_cpu_cores) > 1.5",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Cluster has overcommitted CPU resource requests for Namespaces."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeMemOvercommit",
    "query": "sum(kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",resource=\"memory\",type=\"hard\"}) / sum(kube_node_status_allocatable_memory_bytes{job=\"node-exporter\"}) > 1.5",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Cluster has overcommitted memory resource requests for Namespaces."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeQuotaExceeded",
    "query": "kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",type=\"hard\"} > 0) > 0.9",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "CPUThrottlingHigh",
    "query": "sum by(container, pod, namespace) (increase(container_cpu_cfs_throttled_periods_total{container!=\"\"}[5m])) / sum by(container, pod, namespace) (increase(container_cpu_cfs_periods_total[5m])) > (25 / 100)",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubePersistentVolumeUsageCritical",
    "query": "kubelet_volume_stats_available_bytes{job=\"kubelet\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} < 0.03",
    "duration": 60,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubePersistentVolumeFullInFourDays",
    "query": "(kubelet_volume_stats_available_bytes{job=\"kubelet\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}) < 0.15 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\",namespace=~\"(openshift-.*|kube-.*|default|logging)\"}[6h], 4 * 24 * 3600) < 0",
    "duration": 3600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubePersistentVolumeErrors",
    "query": "kube_persistentvolume_status_phase{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default|logging)\",phase=~\"Failed|Pending\"} > 0",
    "duration": 300,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeClientErrors",
    "query": "(sum by(instance, job) (rate(rest_client_requests_total{code=~\"5..\"}[5m])) / sum by(instance, job) (rate(rest_client_requests_total[5m]))) > 0.01",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeAPILatencyHigh",
    "query": "(cluster:apiserver_request_duration_seconds:mean5m{job=\"apiserver\"} > on(verb) group_left() (avg by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"apiserver\"} >= 0) + 2 * stddev by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"apiserver\"} >= 0))) > on(verb) group_left() 1.2 * avg by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"apiserver\"} >= 0) and on(verb, resource) cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job=\"apiserver\",quantile=\"0.99\"} > 1",
    "duration": 300,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "The API server has an abnormal latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeAPILatencyHigh",
    "query": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job=\"apiserver\",quantile=\"0.99\"} > 4",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeAPIErrorsHigh",
    "query": "sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\"}[5m])) / sum(rate(apiserver_request_total{job=\"apiserver\"}[5m])) > 0.03",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeAPIErrorsHigh",
    "query": "sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\"}[5m])) / sum(rate(apiserver_request_total{job=\"apiserver\"}[5m])) > 0.01",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeAPIErrorsHigh",
    "query": "sum by(resource, subresource, verb) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\"}[5m])) / sum by(resource, subresource, verb) (rate(apiserver_request_total{job=\"apiserver\"}[5m])) > 0.1",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeAPIErrorsHigh",
    "query": "sum by(resource, subresource, verb) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\"}[5m])) / sum by(resource, subresource, verb) (rate(apiserver_request_total{job=\"apiserver\"}[5m])) > 0.05",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeClientCertificateExpiration",
    "query": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and histogram_quantile(0.01, sum by(job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 5400",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "A client certificate used to authenticate to the apiserver is expiring in less than 1.5 hours."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeClientCertificateExpiration",
    "query": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and histogram_quantile(0.01, sum by(job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 3600",
    "duration": 0,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "A client certificate used to authenticate to the apiserver is expiring in less than 1.0 hours."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeAPIDown",
    "query": "absent(up{job=\"apiserver\"} == 1)",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "KubeAPI has disappeared from Prometheus target discovery."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeControllerManagerDown",
    "query": "absent(up{job=\"kube-controller-manager\"} == 1)",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "KubeControllerManager has disappeared from Prometheus target discovery."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeNodeNotReady",
    "query": "kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"} == 0",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "{{ $labels.node }} has been unready for more than 15 minutes."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeNodeUnreachable",
    "query": "kube_node_spec_taint{effect=\"NoSchedule\",job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\"} == 1",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "{{ $labels.node }} is unreachable and some workloads may be rescheduled."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeletTooManyPods",
    "query": "max by(node) (max by(instance) (kubelet_running_pod_count{job=\"kubelet\"}) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\"}) / max by(node) (kube_node_status_capacity_pods{job=\"kube-state-metrics\"}) > 0.95",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeletDown",
    "query": "absent(up{job=\"kubelet\"} == 1)",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "Kubelet has disappeared from Prometheus target discovery."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "KubeSchedulerDown",
    "query": "absent(up{job=\"scheduler\"} == 1)",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "message": "KubeScheduler has disappeared from Prometheus target discovery."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClusterMonitoringOperatorReconciliationErrors",
    "query": "rate(cluster_monitoring_operator_reconcile_errors_total[15m]) * 100 / rate(cluster_monitoring_operator_reconcile_attempts_total[15m]) > 10",
    "duration": 1800,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Cluster Monitoring Operator is experiencing reconciliation error rate of {{ printf \"%0.0f\" $value }}%."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "AlertmanagerReceiversNotConfigured",
    "query": "cluster:alertmanager_routing_enabled:max == 0",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."
    },
    "alerts": [
      {
        "labels": {
          "alertname": "AlertmanagerReceiversNotConfigured",
          "severity": "warning"
        },
        "annotations": {
          "message": "Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."
        },
        "state": "firing",
        "activeAt": "2020-04-18T09:48:06.662770393Z",
        "value": "0e+00"
      }
    ],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeFilesystemSpaceFillingUp",
    "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up.",
      "summary": "Filesystem is predicted to run out of space within the next 24 hours."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeFilesystemSpaceFillingUp",
    "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
    "duration": 3600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.",
      "summary": "Filesystem is predicted to run out of space within the next 4 hours."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeFilesystemAlmostOutOfSpace",
    "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
      "summary": "Filesystem has less than 5% space left."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeFilesystemAlmostOutOfSpace",
    "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
    "duration": 3600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
      "summary": "Filesystem has less than 3% space left."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeFilesystemFilesFillingUp",
    "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.",
      "summary": "Filesystem is predicted to run out of inodes within the next 24 hours."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeFilesystemFilesFillingUp",
    "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
    "duration": 3600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.",
      "summary": "Filesystem is predicted to run out of inodes within the next 4 hours."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeFilesystemAlmostOutOfFiles",
    "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
      "summary": "Filesystem has less than 5% inodes left."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeFilesystemAlmostOutOfFiles",
    "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
    "duration": 3600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
      "summary": "Filesystem has less than 3% inodes left."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeNetworkReceiveErrs",
    "query": "increase(node_network_receive_errs_total[2m]) > 10",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.",
      "summary": "Network interface is reporting many receive errors."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeNetworkTransmitErrs",
    "query": "increase(node_network_transmit_errs_total[2m]) > 10",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.",
      "summary": "Network interface is reporting many transmit errors."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeNetworkInterfaceFlapping",
    "query": "changes(node_network_up{device!~\"veth.+\",job=\"node-exporter\"}[2m]) > 2",
    "duration": 120,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Network interface \"{{ $labels.device }}\" changing it's up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}\""
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClockSkewDetected",
    "query": "abs(node_timex_offset_seconds{job=\"node-exporter\"}) > 0.05",
    "duration": 120,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Clock skew detected on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}. Ensure NTP is configured correctly on this host."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusBadConfig",
    "query": "max_over_time(prometheus_config_last_reload_successful{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) == 0",
    "duration": 600,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.",
      "summary": "Failed Prometheus configuration reload."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusNotificationQueueRunningFull",
    "query": "(predict_linear(prometheus_notifications_queue_length{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]))",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.",
      "summary": "Prometheus alert notification queue predicted to run full in less than 30m."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
    "query": "(rate(prometheus_notifications_errors_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m])) * 100 > 1",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "{{ printf \"%.1f\" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.",
      "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusErrorSendingAlertsToAnyAlertmanager",
    "query": "min without(alertmanager) (rate(prometheus_notifications_errors_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m])) * 100 > 3",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "{{ printf \"%.1f\" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.",
      "summary": "Prometheus encounters more than 3% errors sending alerts to any Alertmanager."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusNotConnectedToAlertmanagers",
    "query": "max_over_time(prometheus_notifications_alertmanagers_discovered{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) < 1",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.",
      "summary": "Prometheus is not connected to any Alertmanagers."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusTSDBReloadsFailing",
    "query": "increase(prometheus_tsdb_reloads_failures_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[3h]) > 0",
    "duration": 14400,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.",
      "summary": "Prometheus has issues reloading blocks from disk."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusTSDBCompactionsFailing",
    "query": "increase(prometheus_tsdb_compactions_failed_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[3h]) > 0",
    "duration": 14400,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.",
      "summary": "Prometheus has issues compacting blocks."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusNotIngestingSamples",
    "query": "rate(prometheus_tsdb_head_samples_appended_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) <= 0",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.",
      "summary": "Prometheus is not ingesting samples."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusDuplicateTimestamps",
    "query": "rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) > 0",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.",
      "summary": "Prometheus is dropping samples with duplicate timestamps."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusOutOfOrderTimestamps",
    "query": "rate(prometheus_target_scrapes_sample_out_of_order_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) > 0",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.",
      "summary": "Prometheus drops samples with out-of-order timestamps."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusRemoteStorageFailures",
    "query": "(rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) / (rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) + rate(prometheus_remote_storage_succeeded_samples_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]))) * 100 > 1",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf \"%.1f\" $value }}% of the samples to queue {{$labels.queue}}.",
      "summary": "Prometheus fails to send samples to remote storage."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusRemoteWriteBehind",
    "query": "(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) - on(job, instance) group_right() max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m])) > 120",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf \"%.1f\" $value }}s behind for queue {{$labels.queue}}.",
      "summary": "Prometheus remote write is behind."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusRemoteWriteDesiredShards",
    "query": "(max_over_time(prometheus_remote_storage_shards_desired{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) > max_over_time(prometheus_remote_storage_shards_max{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]))",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}` $labels.instance | query | first | value }}.",
      "summary": "Prometheus remote write desired shards calculation wants to run more than configured max shards."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusRuleFailures",
    "query": "increase(prometheus_rule_evaluation_failures_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) > 0",
    "duration": 900,
    "labels": {
      "severity": "critical"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.",
      "summary": "Prometheus is failing rule evaluations."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusMissingRuleEvaluations",
    "query": "increase(prometheus_rule_group_iterations_missed_total{job=\"prometheus-k8s\",namespace=\"openshift-monitoring\"}[5m]) > 0",
    "duration": 900,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.",
      "summary": "Prometheus is missing rule evaluations due to slow rule group evaluation."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusOperatorReconcileErrors",
    "query": "rate(prometheus_operator_reconcile_errors_total{job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}[5m]) > 0.1",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Errors while reconciling {{ $labels.controller }} in {{ $labels.namespace }} Namespace."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "PrometheusOperatorNodeLookupErrors",
    "query": "rate(prometheus_operator_node_address_lookup_errors_total{job=\"prometheus-operator\",namespace=\"openshift-monitoring\"}[5m]) > 0.1",
    "duration": 600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "FailingOperator",
    "query": "csv_abnormal{phase=\"Failed\"}",
    "duration": 0,
    "labels": {
      "severity": "info"
    },
    "annotations": {
      "message": "Failed to install Operator {{ $labels.name }} version {{ $labels.version }}. Reason-{{ $labels.reason }}"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeWithoutOVSPod",
    "query": "(kube_node_info unless on(node) kube_pod_info{namespace=\"openshift-sdn\",pod=~\"ovs.*\"}) > 0",
    "duration": 1200,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "All nodes should be running an ovs pod, {{ $labels.node }} is not.\n"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeWithoutSDNPod",
    "query": "(kube_node_info unless on(node) kube_pod_info{namespace=\"openshift-sdn\",pod=~\"sdn.*\"}) > 0",
    "duration": 1200,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "All nodes should be running an sdn pod, {{ $labels.node }} is not.\n"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NetworkPodsCrashLooping",
    "query": "rate(kube_pod_container_status_restarts_total{namespace=\"openshift-sdn\"}[15m]) * 60 * 5 > 0",
    "duration": 3600,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Pod {{ $labels.namespace}}/{{ $labels.pod}} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "IPTableSyncSDNPod",
    "query": "histogram_quantile(0.95, kubeproxy_sync_proxy_rules_duration_seconds_bucket) * on(pod) group_right() kube_pod_info{namespace=\"openshift-sdn\",pod=~\"sdn-[^-]*\"} > 15",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "SDN pod {{ $labels.pod }} on node {{ $labels.node }} takes too long to sync iptables rules."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "IPTableSyncCluster",
    "query": "histogram_quantile(0.95, sum by(le) (rate(kubeproxy_sync_proxy_rules_duration_seconds_bucket[5m]))) > 10",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "The average time for SDN pods to sync iptables is too high."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "NodeIPTablesStale",
    "query": "(timestamp(kubeproxy_sync_proxy_rules_last_timestamp_seconds) - on(pod) kubeproxy_sync_proxy_rules_last_timestamp_seconds) * on(pod) group_right() kube_pod_info{namespace=\"openshift-sdn\",pod=~\"sdn-[^-]*\"} > 120",
    "duration": 1200,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "SDN pod {{ $labels.pod }} on node {{ $labels.node }} has gone too long without syncing iptables rules."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ClusterIPTablesStale",
    "query": "quantile(0.95, timestamp(kubeproxy_sync_proxy_rules_last_timestamp_seconds) - on(pod) kubeproxy_sync_proxy_rules_last_timestamp_seconds * on(pod) group_right() kube_pod_info{namespace=\"openshift-sdn\",pod=~\"sdn-[^-]*\"}) > 90",
    "duration": 1200,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "The average time between iptables resyncs is too high. NOTE - There is some scrape delay and other offsets, 90s isn't exact but it is still too high."
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ServiceCatalogAPIServerEnabled",
    "query": "service_catalog_apiserver_enabled > 0",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Indicates whether Service Catalog API Server is enabled",
      "summary": "Indicates whether Service Catalog API Server is enabled"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  },
  {
    "name": "ServiceCatalogControllerManagerEnabled",
    "query": "service_catalog_controller_manager_enabled > 0",
    "duration": 0,
    "labels": {
      "severity": "warning"
    },
    "annotations": {
      "message": "Indicates whether Service Catalog Controller Manager is enabled",
      "summary": "Indicates whether Service Catalog Controller Manager is enabled"
    },
    "alerts": [],
    "health": "ok",
    "type": "alerting"
  }
]
