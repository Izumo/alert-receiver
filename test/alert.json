do_POST
{"receiver":"webhook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.128.2.4:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"master-2.ocp.example.com","pod":"kube-state-metrics-5745cc99f5-zg64c","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"annotations":{"message":"master-2.ocp.example.com is unreachable and some workloads may be rescheduled."},"startsAt":"2020-03-03T03:03:00.395075553Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=kube_node_spec_taint%7Beffect%3D%22NoSchedule%22%2Cjob%3D%22kube-state-metrics%22%2Ckey%3D%22node.kubernetes.io%2Funreachable%22%7D+%3D%3D+1\u0026g0.tab=1","fingerprint":"9a49c9d8f1a2669b"}],"groupLabels":{"job":"kube-state-metrics"},"commonLabels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.128.2.4:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"master-2.ocp.example.com","pod":"kube-state-metrics-5745cc99f5-zg64c","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"commonAnnotations":{"message":"master-2.ocp.example.com is unreachable and some workloads may be rescheduled."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"kube-state-metrics\"}"}

(u'firing', u'2020-03-03T03:03:00.395075553Z', u'master-2.ocp.example.com is unreachable and some workloads may be rescheduled.')
192.168.11.201 - - [03/Mar/2020 12:04:44] "POST /webhook HTTP/1.1" 200 -
do_POST
{"receiver":"webhook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"annotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"startsAt":"2020-03-03T03:04:15.353892501Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=max+by%28job%29+%28sum+by%28job%29+%28up%7Bjob%3D~%22.%2Aetcd.%2A%22%7D+%3D%3D+bool+0%29+or+count+by%28job%2C+endpoint%29+%28sum+by%28job%2C+endpoint%2C+To%29+%28rate%28etcd_network_peer_sent_failures_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B3m%5D%29%29+%3E+0.01%29%29+%3E+0\u0026g0.tab=1","fingerprint":"676b50699fe8526e"}],"groupLabels":{"job":"etcd"},"commonLabels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"commonAnnotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"etcd\"}"}

(u'firing', u'2020-03-03T03:04:15.353892501Z', u'etcd cluster "etcd": members are down (1).')
192.168.11.201 - - [03/Mar/2020 12:05:30] "POST /webhook HTTP/1.1" 200 -
do_POST
{"receiver":"webhook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"MachineAPIOperatorDown","prometheus":"openshift-monitoring/k8s","severity":"critical"},"annotations":{"message":"machine api operator is down"},"startsAt":"2020-03-03T03:05:01.598262802Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=absent%28up%7Bjob%3D%22machine-api-operator%22%7D+%3D%3D+1%29\u0026g0.tab=1","fingerprint":"93abf4c1950156fb"}],"groupLabels":{},"commonLabels":{"alertname":"MachineAPIOperatorDown","prometheus":"openshift-monitoring/k8s","severity":"critical"},"commonAnnotations":{"message":"machine api operator is down"},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{}"}

(u'firing', u'2020-03-03T03:05:01.598262802Z', u'machine api operator is down')
192.168.11.201 - - [03/Mar/2020 12:08:29] "POST /webhook HTTP/1.1" 200 -
do_POST
{"receiver":"webhook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.128.2.4:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"master-2.ocp.example.com","pod":"kube-state-metrics-5745cc99f5-zg64c","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"annotations":{"message":"master-2.ocp.example.com is unreachable and some workloads may be rescheduled."},"startsAt":"2020-03-03T03:03:00.395075553Z","endsAt":"2020-03-03T03:05:30.395075553Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=kube_node_spec_taint%7Beffect%3D%22NoSchedule%22%2Cjob%3D%22kube-state-metrics%22%2Ckey%3D%22node.kubernetes.io%2Funreachable%22%7D+%3D%3D+1\u0026g0.tab=1","fingerprint":"9a49c9d8f1a2669b"}],"groupLabels":{"job":"kube-state-metrics"},"commonLabels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.128.2.4:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"master-2.ocp.example.com","pod":"kube-state-metrics-5745cc99f5-zg64c","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"commonAnnotations":{"message":"master-2.ocp.example.com is unreachable and some workloads may be rescheduled."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"kube-state-metrics\"}"}

(u'resolved', u'2020-03-03T03:03:00.395075553Z', u'master-2.ocp.example.com is unreachable and some workloads may be rescheduled.')
192.168.11.201 - - [03/Mar/2020 12:09:44] "POST /webhook HTTP/1.1" 200 -
do_POST
{"receiver":"webhook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"annotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"startsAt":"2020-03-03T03:04:15.353892501Z","endsAt":"2020-03-03T03:06:45.353892501Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=max+by%28job%29+%28sum+by%28job%29+%28up%7Bjob%3D~%22.%2Aetcd.%2A%22%7D+%3D%3D+bool+0%29+or+count+by%28job%2C+endpoint%29+%28sum+by%28job%2C+endpoint%2C+To%29+%28rate%28etcd_network_peer_sent_failures_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B3m%5D%29%29+%3E+0.01%29%29+%3E+0\u0026g0.tab=1","fingerprint":"676b50699fe8526e"}],"groupLabels":{"job":"etcd"},"commonLabels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"commonAnnotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"etcd\"}"}

(u'resolved', u'2020-03-03T03:04:15.353892501Z', u'etcd cluster "etcd": members are down (1).')
192.168.11.201 - - [03/Mar/2020 12:10:03] "POST /webhook HTTP/1.1" 200 -
do_POST
{"receiver":"webhook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"annotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"startsAt":"2020-03-03T03:04:15.353892501Z","endsAt":"2020-03-03T03:06:45.353892501Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=max+by%28job%29+%28sum+by%28job%29+%28up%7Bjob%3D~%22.%2Aetcd.%2A%22%7D+%3D%3D+bool+0%29+or+count+by%28job%2C+endpoint%29+%28sum+by%28job%2C+endpoint%2C+To%29+%28rate%28etcd_network_peer_sent_failures_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B3m%5D%29%29+%3E+0.01%29%29+%3E+0\u0026g0.tab=1","fingerprint":"676b50699fe8526e"}],"groupLabels":{"job":"etcd"},"commonLabels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"commonAnnotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"etcd\"}"}

(u'resolved', u'2020-03-03T03:04:15.353892501Z', u'etcd cluster "etcd": members are down (1).')
192.168.11.201 - - [03/Mar/2020 12:10:30] "POST /webhook HTTP/1.1" 200 -
do_POST
{"receiver":"webhook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"MachineAPIOperatorDown","prometheus":"openshift-monitoring/k8s","severity":"critical"},"annotations":{"message":"machine api operator is down"},"startsAt":"2020-03-03T03:05:01.598262802Z","endsAt":"2020-03-03T03:06:31.598262802Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=absent%28up%7Bjob%3D%22machine-api-operator%22%7D+%3D%3D+1%29\u0026g0.tab=1","fingerprint":"93abf4c1950156fb"}],"groupLabels":{},"commonLabels":{"alertname":"MachineAPIOperatorDown","prometheus":"openshift-monitoring/k8s","severity":"critical"},"commonAnnotations":{"message":"machine api operator is down"},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{}"}




{"receiver":"web hook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.130.0.8:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"master-2.ocp.example.com","pod":"kube-state-metrics-5bc9b987bc-zpfzl","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"annotations":{"message":"master-2.ocp.example.com is unreachable and some workloads may be rescheduled."},"startsAt":"2020-04-28T01:40:30.395075553Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=kube_node_spec_taint%7Beffect%3D%22NoSchedule%22%2Cjob%3D%22kube-state-metrics%22%2Ckey%3D%22node.kubernetes.io%2Funreachable%22%7D+%3D%3D+1\u0026g0.tab=1","fingerprint":"4e8e5546ea7cd258"}],"groupLabels":{"job":"kube-state-metrics"},"commonLabels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.130.0.8:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"master-2.ocp.example.com","pod":"kube-state-metrics-5bc9b987bc-zpfzl","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"commonAnnotations":{"message":"master-2.ocp.example.com is unreachable and some workloads may be rescheduled."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"kube-state-metrics\"}"}


{"receiver":"web hook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"annotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"startsAt":"2020-04-28T01:42:45.353892501Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=max+by%28job%29+%28sum+by%28job%29+%28up%7Bjob%3D~%22.%2Aetcd.%2A%22%7D+%3D%3D+bool+0%29+or+count+by%28job%2C+endpoint%29+%28sum+by%28job%2C+endpoint%2C+To%29+%28rate%28etcd_network_peer_sent_failures_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B3m%5D%29%29+%3E+0.01%29%29+%3E+0\u0026g0.tab=1","fingerprint":"676b50699fe8526e"}],"groupLabels":{"job":"etcd"},"commonLabels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"commonAnnotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"etcd\"}"}



{"receiver":"web hook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.130.0.8:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"master-2.ocp.example.com","pod":"kube-state-metrics-5bc9b987bc-zpfzl","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"annotations":{"message":"master-2.ocp.example.com is unreachable and some workloads may be rescheduled."},"startsAt":"2020-04-28T01:40:30.395075553Z","endsAt":"2020-04-28T01:43:30.395075553Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=kube_node_spec_taint%7Beffect%3D%22NoSchedule%22%2Cjob%3D%22kube-state-metrics%22%2Ckey%3D%22node.kubernetes.io%2Funreachable%22%7D+%3D%3D+1\u0026g0.tab=1","fingerprint":"4e8e5546ea7cd258"}],"groupLabels":{"job":"kube-state-metrics"},"commonLabels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.130.0.8:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"master-2.ocp.example.com","pod":"kube-state-metrics-5bc9b987bc-zpfzl","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"commonAnnotations":{"message":"master-2.ocp.example.com is unreachable and some workloads may be rescheduled."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"kube-state-metrics\"}"}


{"receiver":"web hook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"annotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"startsAt":"2020-04-28T01:42:45.353892501Z","endsAt":"2020-04-28T01:44:45.353892501Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=max+by%28job%29+%28sum+by%28job%29+%28up%7Bjob%3D~%22.%2Aetcd.%2A%22%7D+%3D%3D+bool+0%29+or+count+by%28job%2C+endpoint%29+%28sum+by%28job%2C+endpoint%2C+To%29+%28rate%28etcd_network_peer_sent_failures_total%7Bjob%3D~%22.%2Aetcd.%2A%22%7D%5B3m%5D%29%29+%3E+0.01%29%29+%3E+0\u0026g0.tab=1","fingerprint":"676b50699fe8526e"}],"groupLabels":{"job":"etcd"},"commonLabels":{"alertname":"etcdMembersDown","job":"etcd","prometheus":"openshift-monitoring/k8s","severity":"critical"},"commonAnnotations":{"message":"etcd cluster \"etcd\": members are down (1)."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\"etcd\"}"}


{"receiver":"web hook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"AlertmanagerReceiversNotConfigured","prometheus":"openshift-monitoring/k8s","severity":"warning"},"annotations":{"message":"Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."},"startsAt":"2020-04-28T01:58:36.662770393Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=cluster%3Aalertmanager_routing_enabled%3Amax+%3D%3D+0\\u0026g0.tab=1","fingerprint":"14298351083980ef"}],"groupLabels":{},"commonLabels":{"alertname":"AlertmanagerReceiversNotConfigured","prometheus":"openshift-monitoring/k8s","severity":"warning"},"commonAnnotations":{"message":"Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{}



{"receiver":"web hook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.130.0.8:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"worker-1.ocp.example.com","pod":"kube-state-metrics-5bc9b987bc-zpfzl","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"annotations":{"message":"worker-1.ocp.example.com is unreachable and some workloads may be rescheduled."},"startsAt":"2020-04-28T01:59:00.395075553Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=kube_node_spec_taint%7Beffect%3D%22NoSchedule%22%2Cjob%3D%22kube-state-metrics%22%2Ckey%3D%22node.kubernetes.io%2Funreachable%22%7D+%3D%3D+1\\u0026g0.tab=1","fingerprint":"c4de53561714f38b"}],"groupLabels":{"job":"kube-state-metrics"},"commonLabels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.130.0.8:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"worker-1.ocp.example.com","pod":"kube-state-metrics-5bc9b987bc-zpfzl","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"commonAnnotations":{"message":"worker-1.ocp.example.com is unreachable and some workloads may be rescheduled."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\\"kube-state-metrics\\"}"}

{"receiver":"web hook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"AlertmanagerReceiversNotConfigured","prometheus":"openshift-monitoring/k8s","severity":"warning"},"annotations":{"message":"Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."},"startsAt":"2020-04-28T01:58:36.662770393Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=cluster%3Aalertmanager_routing_enabled%3Amax+%3D%3D+0\\u0026g0.tab=1","fingerprint":"14298351083980ef"}],"groupLabels":{},"commonLabels":{"alertname":"AlertmanagerReceiversNotConfigured","prometheus":"openshift-monitoring/k8s","severity":"warning"},"commonAnnotations":{"message":"Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{}

{"receiver":"web hook","status":"firing","alerts":[{"status":"firing","labels":{"alertname":"KubeAPILatencyHigh","component":"apiserver","endpoint":"https","job":"apiserver","namespace":"default","prometheus":"openshift-monitoring/k8s","resource":"events","scope":"namespace","service":"kubernetes","severity":"warning","verb":"POST","version":"v1"},"annotations":{"message":"The API server has an abnormal latency of 0.4064638268785913 seconds for POST events."},"startsAt":"2020-04-28T02:01:20.907806203Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E+on%28verb%29+group_left%28%29+%28avg+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29+%2B+2+%2A+stddev+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29%29%29+%3E+on%28verb%29+group_left%28%29+1.2+%2A+avg+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29+and+on%28verb%2C+resource%29+cluster_quantile%3Aapiserver_request_duration_seconds%3Ahistogram_quantile%7Bjob%3D%22apiserver%22%2Cquantile%3D%220.99%22%7D+%3E+1\\u0026g0.tab=1","fingerprint":"aba669429a849c6f"}],"groupLabels":{"job":"apiserver"},"commonLabels":{"alertname":"KubeAPILatencyHigh","component":"apiserver","endpoint":"https","job":"apiserver","namespace":"default","prometheus":"openshift-monitoring/k8s","resource":"events","scope":"namespace","service":"kubernetes","severity":"warning","verb":"POST","version":"v1"},"commonAnnotations":{"message":"The API server has an abnormal latency of 0.4064638268785913 seconds for POST events."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\\"apiserver\\"}"}


{"alertname":"AlertmanagerReceiversNotConfigured","prometheus":"openshift-monitoring/k8s","severity":"warning"},"annotations":{"message":"Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."},"startsAt":"2020-04-28T01:58:36.662770393Z","endsAt":"2020-04-28T02:01:36.662770393Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=cluster%3Aalertmanager_routing_enabled%3Amax+%3D%3D+0\\u0026g0.tab=1","fingerprint":"14298351083980ef"},{"status":"firing","labels":{"alertname":"AlertmanagerConfigInconsistent","config_hash":"160185150117055","prometheus":"openshift-monitoring/k8s","service":"alertmanager-main","severity":"critical"},"annotations":{"message":"The configuration of the instances of the Alertmanager cluster `alertmanager-main` are out of sync."},"startsAt":"2020-04-28T02:03:34.485794182Z","endsAt":"0001-01-01T00:00:00Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=count_values+by%28service%29+%28%22config_hash%22%2C+alertmanager_config_hash%7Bjob%3D%22alertmanager-main%22%2Cnamespace%3D%22openshift-monitoring%22%7D%29+%2F+on%28service%29+group_left%28%29+label_replace%28max+by%28name%2C+job%2C+namespace%2C+controller%29+%28prometheus_operator_spec_replicas%7Bcontroller%3D%22alertmanager%22%2Cjob%3D%22prometheus-operator%22%2Cnamespace%3D%22openshift-monitoring%22%7D%29%2C+%22service%22%2C+%22alertmanager-%241%22%2C+%22name%22%2C+%22%28.%2A%29%22%29+%21%3D+1\\u0026g0.tab=1","fingerprint":"ad158a0768714051"}],"groupLabels":{},"commonLabels":{"prometheus":"openshift-monitoring/k8s"},"commonAnnotations":{},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{}"}

{"receiver":"web hook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.130.0.8:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"worker-1.ocp.example.com","pod":"kube-state-metrics-5bc9b987bc-zpfzl","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"annotations":{"message":"worker-1.ocp.example.com is unreachable and some workloads may be rescheduled."},"startsAt":"2020-04-28T01:59:00.395075553Z","endsAt":"2020-04-28T02:03:30.395075553Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=kube_node_spec_taint%7Beffect%3D%22NoSchedule%22%2Cjob%3D%22kube-state-metrics%22%2Ckey%3D%22node.kubernetes.io%2Funreachable%22%7D+%3D%3D+1\\u0026g0.tab=1","fingerprint":"c4de53561714f38b"}],"groupLabels":{"job":"kube-state-metrics"},"commonLabels":{"alertname":"KubeNodeUnreachable","effect":"NoSchedule","endpoint":"https-main","instance":"10.130.0.8:8443","job":"kube-state-metrics","key":"node.kubernetes.io/unreachable","namespace":"openshift-monitoring","node":"worker-1.ocp.example.com","pod":"kube-state-metrics-5bc9b987bc-zpfzl","prometheus":"openshift-monitoring/k8s","service":"kube-state-metrics","severity":"warning"},"commonAnnotations":{"message":"worker-1.ocp.example.com is unreachable and some workloads may be rescheduled."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\\"kube-state-metrics\\"}"}

{"receiver":"web hook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"AlertmanagerReceiversNotConfigured","prometheus":"openshift-monitoring/k8s","severity":"warning"},"annotations":{"message":"Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."},"startsAt":"2020-04-28T01:58:36.662770393Z","endsAt":"2020-04-28T02:01:36.662770393Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=cluster%3Aalertmanager_routing_enabled%3Amax+%3D%3D+0\\u0026g0.tab=1","fingerprint":"14298351083980ef"}],"groupLabels":{},"commonLabels":{"alertname":"AlertmanagerReceiversNotConfigured","prometheus":"openshift-monitoring/k8s","severity":"warning"},"commonAnnotations":{"message":"Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager."},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{}"}



{"receiver":"web hook","status":"resolved","alerts":[{"status":"resolved","labels":{"alertname":"KubeAPILatencyHigh","component":"apiserver","endpoint":"https","job":"apiserver","namespace":"default","prometheus":"openshift-monitoring/k8s","resource":"events","scope":"namespace","service":"kubernetes","severity":"warning","verb":"POST","version":"v1"},"annotations":{"message":"The API server has an abnormal latency of 0.4104030684350182 seconds for POST events."},"startsAt":"2020-04-28T02:01:20.907806203Z","endsAt":"2020-04-28T02:02:50.907806203Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E+on%28verb%29+group_left%28%29+%28avg+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29+%2B+2+%2A+stddev+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29%29%29+%3E+on%28verb%29+group_left%28%29+1.2+%2A+avg+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29+and+on%28verb%2C+resource%29+cluster_quantile%3Aapiserver_request_duration_seconds%3Ahistogram_quantile%7Bjob%3D%22apiserver%22%2Cquantile%3D%220.99%22%7D+%3E+1\\u0026g0.tab=1","fingerprint":"aba669429a849c6f"},{"status":"resolved","labels":{"alertname":"KubeAPILatencyHigh","component":"apiserver","endpoint":"https","job":"apiserver","namespace":"default","prometheus":"openshift-monitoring/k8s","resource":"pods","scope":"namespace","service":"kubernetes","severity":"warning","verb":"GET","version":"v1"},"annotations":{"message":"The API server has an abnormal latency of 0.03474829911215911 seconds for GET pods."},"startsAt":"2020-04-28T02:06:20.907806203Z","endsAt":"2020-04-28T02:06:50.907806203Z","generatorURL":"https://prometheus-k8s-openshift-monitoring.apps.ocp.example.com/graph?g0.expr=%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E+on%28verb%29+group_left%28%29+%28avg+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29+%2B+2+%2A+stddev+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29%29%29+%3E+on%28verb%29+group_left%28%29+1.2+%2A+avg+by%28verb%29+%28cluster%3Aapiserver_request_duration_seconds%3Amean5m%7Bjob%3D%22apiserver%22%7D+%3E%3D+0%29+and+on%28verb%2C+resource%29+cluster_quantile%3Aapiserver_request_duration_seconds%3Ahistogram_quantile%7Bjob%3D%22apiserver%22%2Cquantile%3D%220.99%22%7D+%3E+1\\u0026g0.tab=1","fingerprint":"3ed4af79b651b39c"}],"groupLabels":{"job":"apiserver"},"commonLabels":{"alertname":"KubeAPILatencyHigh","component":"apiserver","endpoint":"https","job":"apiserver","namespace":"default","prometheus":"openshift-monitoring/k8s","scope":"namespace","service":"kubernetes","severity":"warning","version":"v1"},"commonAnnotations":{},"externalURL":"https://alertmanager-main-openshift-monitoring.apps.ocp.example.com","version":"4","groupKey":"{}:{job=\\"apiserver\\"}"}



